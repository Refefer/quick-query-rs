# Full Configuration Example
# Copy to: ~/.config/qq/config.toml
#
# This shows all available configuration options.

# Default profile to use (required)
default_profile = "default"

# =============================================================================
# Providers - API credentials and settings
# =============================================================================

# OpenAI Configuration
[providers.openai]
api_key = "sk-your-openai-api-key-here"
base_url = "https://api.openai.com/v1"  # Optional, this is the default
default_model = "gpt-4o"                # Override global default for this provider

# Extra parameters passed to the API (provider-specific)
[providers.openai.parameters]
# Examples of custom parameters:
# reasoning_effort = "medium"                    # For o1/reasoning models
# chat_template_kwargs = { reasoning_effort = "high" }  # For custom templates

# Anthropic Configuration (native provider)
# Type is auto-detected from provider name "anthropic"
[providers.anthropic]
api_key = "sk-ant-your-anthropic-key-here"  # Or set ANTHROPIC_API_KEY env var
default_model = "claude-sonnet-4-20250514"

# Google Gemini Configuration (native provider)
# Type is auto-detected from provider name "gemini"
[providers.gemini]
api_key = "your-gemini-api-key-here"  # Or set GEMINI_API_KEY env var
default_model = "gemini-2.5-flash"

# Custom-named provider with explicit type
# Use `type` when your provider name doesn't match the provider type
[providers.my-claude]
type = "anthropic"
api_key = "sk-ant-your-key"
base_url = "https://my-proxy.example.com/v1"  # Optional proxy

# Mistral Configuration (for future use)
[providers.mistral]
api_key = "your-mistral-api-key-here"
default_model = "mistral-large-latest"

# =============================================================================
# Profiles - bundle provider + prompt + model + parameters
# =============================================================================

[profiles.default]
provider = "openai"
model = "gpt-4o"

[profiles.coding]
provider = "openai"
prompt = "coder"  # References [prompts.coder]
model = "gpt-4o"

[profiles.reasoning]
provider = "openai"
model = "o1"
[profiles.reasoning.parameters]
reasoning_effort = "high"

# =============================================================================
# Prompts - reusable system prompts
# =============================================================================

[prompts.coder]
prompt = "You are an expert programmer. Write clean, efficient code with clear explanations."

[prompts.concise]
prompt = "Be concise and direct. No unnecessary explanations."

# =============================================================================
# Tools Configuration
# =============================================================================
# Configure the built-in tools (filesystem, memory, web)
# Tools are always enabled

[tools]
# Root directory for filesystem operations
# Supports: $PWD, $HOME, ~, ${VAR}
# Default: current working directory ($PWD)
root = "$PWD"

# Path to SQLite database for persistent memory
# Default: ~/.config/qq/memory.db
memory_db = "$HOME/.config/qq/memory.db"

# Allow write operations (write_file tool)
# Default: true
allow_write = true

# Enable/disable specific tool categories
enable_web = true
enable_filesystem = true
enable_memory = true

# =============================================================================
# Compaction - Observational Memory
# =============================================================================
# During long conversations, older messages are automatically distilled into a
# structured observation log by an "Observer" LLM pass, keeping the context
# window manageable. When the log itself grows large, a "Reflector" LLM pass
# merges and compresses it.
#
# This applies to the main chat session. Agents have their own context management
# controlled per-agent via memory_strategy in agents.toml:
#   - "obs-memory"  (default for built-in agents) - same observational memory,
#     running inside the agent loop with agent-tuned thresholds (30KB/100KB).
#   - "compaction"  (default for external agents) - post-execution LLM
#     summarization with continuation support.
# See agents.toml for per-agent configuration.
#
# The defaults work well for most use cases. Only change these if you have a
# specific reason â€” incorrect values can cause excessive LLM calls (thresholds
# too low) or blow through context limits (thresholds too high).

[compaction]
# Provider for compaction LLM calls (defaults to the session provider).
# Set this to route compaction through a cheaper/faster provider.
# This provider is shared with agents using obs-memory.
# provider = "anthropic"

# Model override for compaction LLM calls.
# When set, compaction uses this model instead of the session model.
# A smaller, cheaper model usually works fine here.
# model = "claude-3-5-haiku"

# Byte threshold for unobserved messages before the Observer triggers.
# When unobserved messages exceed this size, they are distilled into observations.
# Default: 50000 (50 KB) for chat, 30000 (30 KB) for agents
# message_threshold_bytes = 50000

# Byte threshold for the observation log before the Reflector triggers.
# When the observation log exceeds this size, it is merged and compressed.
# Default: 200000 (200 KB) for chat, 100000 (100 KB) for agents
# observation_threshold_bytes = 200000

# Number of recent messages to always keep verbatim (never observe).
# These are sent as-is to the LLM for full fidelity on recent exchanges.
# Default: 10 for chat, 6 for agents
# preserve_recent = 10

# Hysteresis multiplier applied to thresholds to prevent compaction from
# re-triggering immediately after a pass. Effective threshold = threshold * hysteresis.
# Values slightly above 1.0 work best. Default: 1.1
# hysteresis = 1.1
